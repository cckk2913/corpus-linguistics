{"id":232245521,"title":"#分享 吃角子老虎(Bandit) 問題 - Multiarm Bandit","content":"最近依然在跟推薦系統奮鬥，中間我們做出了好幾種 poc，在 AB testing 的過程當中，怎樣分配流量才能夠不太損失公司的利益，又能同時檢驗出哪個 poc 效果最好呢？其中一種做法就是 Multi-armed bandit\n\nhttps://g8787856.github.io/img/rl_learning/bandit.png\nMulti-armed bandit原本是從賭場中的多臂老虎機的場景中提取出來的數學模型，其中 arm 指的是老虎機（slot machine）的拉桿，bandit 是多個拉桿的集合\n\nhttps://cdn-images-1.medium.com/max/1400/1*Tt8A6mP98ibBlrlFD5UJxg.png\n大家可以想像賭場有四台吃角子老虎機，每一台賺錢的機率如上圖所示，那怎樣才能轉隊多錢呢？想必是一直去玩左邊那台老虎機對吧？但是這些機率其實是你量出來的，我們並不知道他真正的機率，只能自己去多玩幾次來求得一個逼近真實的機率。那你可能會想問，這張圖顯示的機率真的準嗎？好問題，不知道🤔，這個機率有可能是有 bias 的，所以得要多方嘗試其他的老虎機來極大化收益。\n\n以 epsilon=10%-greedy 為例子，就是 90% 的時間都去玩左邊的老虎機，10% 的時間去探索新的老虎機看看他們的收益有沒有比較高，90% 的行為被稱作 exploit （greedy 的行為），10 % 的行為被稱作 explore （因為是去探索有沒有更高收益的機器）。比例是可以調整的，可以完全的 exploit or explore\n\n每一輪探索玩老虎機之後，我們怎麼更新老虎機的機率呢（也就是收益，reward）？\n這邊有各種實作，目前我看到最屌的是結合 Q-Learning 的\b action-value function 來更新 reward（請看下圖）。查了一下才發現，Multi-armed bandits 是 reinforcement learning 的一員，論資歷還比 Q-Learning 資深呢XD\n\nhttp://i.imgur.com/L3gm9ht.png\n\n今天分享比較簡短，想看詳細實作的可以參考下方的 reference 喔～\n\nreference: https://towardsdatascience.com/solving-the-multi-armed-bandit-problem-b72de40db97c\ngithub: https://github.com/ankonzoid/LearningX/blob/master/classical_RL/MAB/MAB.py#L70\nQ-Learning: https://www.zhihu.com/question/26408259","excerpt":"最近依然在跟推薦系統奮鬥，中間我們做出了好幾種 poc，在 AB testing 的過程當中，怎樣分配流量才能夠不太損失公司的利益，又能同時檢驗出哪個 poc 效果最好呢？其中一種做法就是 Multi","anonymousSchool":false,"anonymousDepartment":false,"pinned":false,"forumId":"a6dc1b6b-0aca-4004-86c8-3af6cdbfad23","replyId":null,"createdAt":"2019-10-06T16:32:03.024Z","updatedAt":"2019-10-06T16:32:03.024Z","commentCount":4,"likeCount":12,"tags":[],"topics":["bandit","強化學習","增強學習","ab測試","abtesting"],"supportedReactions":null,"withNickname":true,"reportReason":"","hiddenByAuthor":false,"meta":{"layout":"classic"},"forumName":"軟體工程師","forumAlias":"softwareengineer","school":"搜尋工程師 小david","department":"ggggghgggg","replyTitle":null,"gender":"M","personaSubscriptable":true,"reactions":[{"id":"286f599c-f86a-4932-82f0-f5a06f1eca03","count":11},{"id":"aa0d425f-d530-4478-9a77-fe3aedc79eea","count":1}],"hidden":false,"customStyle":null,"isSuspiciousAccount":false,"layout":"classic","withImages":true,"withVideos":false,"media":[{"url":"https://g8787856.github.io/img/rl_learning/bandit.png"},{"url":"https://cdn-images-1.medium.com/max/1400/1*Tt8A6mP98ibBlrlFD5UJxg.png"},{"url":"http://i.imgur.com/L3gm9ht.png"}],"reportReasonText":"","mediaMeta":[{"id":"ba27522a-b699-4fe2-9c68-826408b7aca8","url":"https://g8787856.github.io/img/rl_learning/bandit.png","normalizedUrl":"https://g8787856.github.io/img/rl_learning/bandit.png","thumbnail":"https://g8787856.github.io/img/rl_learning/bandit.png","type":"image/thumbnail","tags":["ANNOTATED"],"createdAt":"2019-10-06T16:32:03.024Z","updatedAt":"2019-10-06T16:32:03.024Z"},{"id":"ba27522a-b699-4fe2-9c68-826408b7aca8","url":"https://g8787856.github.io/img/rl_learning/bandit.png","normalizedUrl":"","thumbnail":"https://g8787856.github.io/img/rl_learning/bandit.png","type":"image/png","tags":["ANNOTATED"],"createdAt":"2019-10-06T16:32:03.024Z","updatedAt":"2019-10-06T16:32:03.024Z"},{"id":"01307130-3fb9-4858-9975-c9da2757dd59","url":"https://cdn-images-1.medium.com/max/1400/1*Tt8A6mP98ibBlrlFD5UJxg.png","normalizedUrl":"","thumbnail":"https://cdn-images-1.medium.com/max/1400/1*Tt8A6mP98ibBlrlFD5UJxg.png","type":"image/png","tags":["ANNOTATED"],"createdAt":"2019-10-06T16:32:03.024Z","updatedAt":"2019-10-06T16:32:03.024Z"},{"id":"7e06da7d-9aed-4a7d-b8dd-823536f57948","url":"http://i.imgur.com/L3gm9ht.png","normalizedUrl":"https://imgur.com/L3gm9ht","thumbnail":"https://i.imgur.com/L3gm9htl.jpg","type":"image/imgur","tags":["ANNOTATED"],"createdAt":"2019-10-06T16:32:03.024Z","updatedAt":"2019-10-06T16:32:03.024Z"}],"postAvatar":""}